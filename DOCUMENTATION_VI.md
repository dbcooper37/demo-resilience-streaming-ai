# ğŸš€ Há»‡ Thá»‘ng AI Chat Äa Node - TÃ i Liá»‡u POC

**PhiÃªn báº£n:** 1.0  
**NgÃ y:** ThÃ¡ng 11/2025  
**Tráº¡ng thÃ¡i:** Proof of Concept

---

## ğŸ“‹ Má»¥c Lá»¥c

1. [TÃ³m Táº¯t Äiá»u HÃ nh](#tÃ³m-táº¯t-Ä‘iá»u-hÃ nh)
2. [Kiáº¿n TrÃºc Há»‡ Thá»‘ng](#kiáº¿n-trÃºc-há»‡-thá»‘ng)
3. [CÃ¡c ThÃ nh Pháº§n ChÃ­nh](#cÃ¡c-thÃ nh-pháº§n-chÃ­nh)
4. [Luá»“ng Xá»­ LÃ½](#luá»“ng-xá»­-lÃ½)
5. [TÃ­nh NÄƒng ChÃ­nh](#tÃ­nh-nÄƒng-chÃ­nh)
6. [Triá»ƒn Khai Ká»¹ Thuáº­t](#triá»ƒn-khai-ká»¹-thuáº­t)
7. [Kiáº¿n TrÃºc Triá»ƒn Khai](#kiáº¿n-trÃºc-triá»ƒn-khai)
8. [Hiá»‡u NÄƒng & Kháº£ NÄƒng Má»Ÿ Rá»™ng](#hiá»‡u-nÄƒng--kháº£-nÄƒng-má»Ÿ-rá»™ng)
9. [Táº¡i Sao Chá»n Giáº£i PhÃ¡p NÃ y](#táº¡i-sao-chá»n-giáº£i-phÃ¡p-nÃ y)
10. [Khá»Ÿi Äá»™ng Nhanh](#khá»Ÿi-Ä‘á»™ng-nhanh)

---

## ğŸ“Š TÃ³m Táº¯t Äiá»u HÃ nh

### BÃ i ToÃ¡n

XÃ¢y dá»±ng há»‡ thá»‘ng chat AI cÃ³ kháº£ nÄƒng má»Ÿ rá»™ng, thá»i gian thá»±c vá»›i cÃ¡c yÃªu cáº§u:
- **Pháº£n há»“i streaming thá»i gian thá»±c** tá»« mÃ´ hÃ¬nh AI
- **Triá»ƒn khai Ä‘a node** Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh kháº£ dá»¥ng cao
- **Káº¿t ná»‘i WebSocket liÃªn tá»¥c** trÃªn cÃ¡c node phÃ¢n tÃ¡n
- **Shared state** trÃªn táº¥t cáº£ cÃ¡c instances á»©ng dá»¥ng
- **CÃ¢n báº±ng táº£i** vá»›i session affinity

### Giáº£i PhÃ¡p Tá»•ng Quan

Kiáº¿n trÃºc phÃ¢n tÃ¡n, hÆ°á»›ng sá»± kiá»‡n táº­n dá»¥ng:
- **Sticky Sessions** qua Nginx `ip_hash` cho WebSocket persistence
- **Redis PubSub** cho phÃ¢n phá»‘i message thá»i gian thá»±c
- **Distributed Locks** (Redisson) cho tÃ­nh nháº¥t quÃ¡n dá»¯ liá»‡u
- **Backend API Gateway** cho truy cáº­p AI service táº­p trung
- **Round-Robin Load Balancing** cho cÃ¡c request AI service

### ThÃ nh Tá»±u ChÃ­nh

| Chá»‰ Sá»‘ | GiÃ¡ Trá»‹ | MÃ´ Táº£ |
|--------|---------|-------|
| **Availability** | 99.9%+ | Triá»ƒn khai Ä‘a node vá»›i health checks |
| **Scalability** | Horizontal | ThÃªm nodes khÃ´ng cáº§n thay Ä‘á»•i code |
| **Latency** | <100ms | Streaming real-time qua Redis PubSub |
| **Consistency** | Strong | Distributed locks Ä‘áº£m báº£o tÃ­nh toÃ n váº¹n |
| **Session Affinity** | 100% | Sticky sessions duy trÃ¬ káº¿t ná»‘i WebSocket |

---

## ğŸ—ï¸ Kiáº¿n TrÃºc Há»‡ Thá»‘ng

### Kiáº¿n TrÃºc Tá»•ng QuÃ¡t

```mermaid
graph TB
    subgraph "Táº§ng Client"
        C1[React Frontend 1]
        C2[React Frontend 2]
        C3[React Frontend N]
    end

    subgraph "Load Balancer"
        LB[Nginx Load Balancer<br/>ip_hash cho Sticky Sessions]
    end

    subgraph "Backend Cluster - Java WebSocket Servers"
        WS1[WebSocket Node 1<br/>Port 8080]
        WS2[WebSocket Node 2<br/>Port 8080]
        WS3[WebSocket Node 3<br/>Port 8080]
    end

    subgraph "AI Service Cluster - Python FastAPI"
        AI1[AI Service Node 1<br/>Port 8000]
        AI2[AI Service Node 2<br/>Port 8000]
        AI3[AI Service Node 3<br/>Port 8000]
    end

    subgraph "Háº¡ Táº§ng Chia Sáº»"
        REDIS[(Redis<br/>PubSub + Cache)]
        KAFKA[(Kafka<br/>Event Sourcing)]
        DB[(H2 Database<br/>LÆ°u trá»¯ Message)]
    end

    C1 --> LB
    C2 --> LB
    C3 --> LB

    LB -->|Sticky Session| WS1
    LB -->|Sticky Session| WS2
    LB -->|Sticky Session| WS3

    WS1 -->|Round-Robin| AI1
    WS1 -->|Round-Robin| AI2
    WS1 -->|Round-Robin| AI3

    WS2 -->|Round-Robin| AI1
    WS2 -->|Round-Robin| AI2
    WS2 -->|Round-Robin| AI3

    WS3 -->|Round-Robin| AI1
    WS3 -->|Round-Robin| AI2
    WS3 -->|Round-Robin| AI3

    WS1 --> REDIS
    WS2 --> REDIS
    WS3 --> REDIS

    AI1 --> REDIS
    AI2 --> REDIS
    AI3 --> REDIS

    WS1 --> KAFKA
    WS2 --> KAFKA
    WS3 --> KAFKA

    WS1 --> DB
    WS2 --> DB
    WS3 --> DB

    style LB fill:#ff9999
    style REDIS fill:#ffcc99
    style KAFKA fill:#99ccff
    style DB fill:#cc99ff
```

### CÃ¡c Táº§ng Kiáº¿n TrÃºc

#### 1. **Táº§ng Client**
- á»¨ng dá»¥ng frontend React + Vite
- Káº¿t ná»‘i WebSocket cho streaming real-time
- Gá»i REST API cho cÃ¡c thao tÃ¡c chat

#### 2. **Táº§ng Load Balancer**
- Nginx vá»›i directive `ip_hash`
- Route káº¿t ná»‘i WebSocket vá»›i sticky sessions
- Proxy cÃ¡c API calls qua backend

#### 3. **Táº§ng Backend Cluster**
- Java Spring Boot WebSocket servers
- Quáº£n lÃ½ session phÃ¢n tÃ¡n qua Redis
- API Gateway cho truy cáº­p AI service
- Load balancing ná»™i bá»™ tá»›i AI services

#### 4. **Táº§ng AI Service Cluster**
- Python FastAPI microservices
- TÃ­ch há»£p mÃ´ hÃ¬nh AI
- Redis PubSub cho streaming responses

#### 5. **Táº§ng Háº¡ Táº§ng**
- **Redis**: Shared state, PubSub, distributed locks
- **Kafka**: Event sourcing vÃ  analytics
- **H2 Database**: Message persistence

---

## ğŸ”§ CÃ¡c ThÃ nh Pháº§n ChÃ­nh

### 1. Session Manager (Java)

**Má»¥c Ä‘Ã­ch:** Quáº£n lÃ½ WebSocket sessions phÃ¢n tÃ¡n trÃªn nhiá»u backend nodes

**TÃ­nh nÄƒng chÃ­nh:**
- Registry session phÃ¢n tÃ¡n trong Redis
- GiÃ¡m sÃ¡t heartbeat
- Tá»± Ä‘á»™ng dá»n dáº¹p sessions cÅ©

```mermaid
graph LR
    A[Káº¿t Ná»‘i WebSocket] --> B[SessionManager]
    B --> C{ÄÄƒng KÃ½ Session}
    C --> D[Bá»™ Nhá»› Local<br/>ConcurrentHashMap]
    C --> E[Redis<br/>Registry PhÃ¢n TÃ¡n]
    E --> F[sessions:active]
    E --> G[sessions:user:userId]
    
    style B fill:#99ccff
    style E fill:#ffcc99
```

**Äiá»ƒm ná»•i báº­t triá»ƒn khai:**
```java
// Tracking session phÃ¢n tÃ¡n
RMap<String, String> activeSessionsMap = redissonClient.getMap("sessions:active");
activeSessionsMap.put(sessionId, userId);

// Sessions theo user cá»¥ thá»ƒ
RSet<String> userSessions = redissonClient.getSet("sessions:user:" + userId);
userSessions.add(sessionId);
```

### 2. Redis Stream Cache (Java)

**Má»¥c Ä‘Ã­ch:** Cache streaming chunks vá»›i Ä‘áº£m báº£o thá»© tá»±

**TÃ­nh nÄƒng chÃ­nh:**
- Distributed locks cho thá»© tá»± chunks
- Redis List cho lÆ°u trá»¯ tuáº§n tá»±
- Quáº£n lÃ½ TTL tá»± Ä‘á»™ng

```mermaid
sequenceDiagram
    participant WS as WebSocket Handler
    participant Cache as RedisStreamCache
    participant Redisson as Distributed Lock
    participant Redis as Redis List

    WS->>Cache: appendChunk(messageId, chunk)
    Cache->>Redisson: tryLock(messageId)
    Redisson-->>Cache: lock acquired
    Cache->>Redis: RPUSH chunks:messageId
    Cache->>Redis: SET TTL 5min
    Cache->>Redisson: unlock()
    Redisson-->>WS: chunk appended
```

**Táº¡i sao cáº§n Distributed Locks?**
- Nhiá»u backend nodes cÃ³ thá»ƒ nháº­n chunks khÃ´ng Ä‘Ãºng thá»© tá»±
- Lock Ä‘áº£m báº£o thao tÃ¡c append tuáº§n tá»±
- NgÄƒn cháº·n data race conditions

### 3. Chat Orchestrator (Java)

**Má»¥c Ä‘Ã­ch:** Äiá»u phá»‘i streaming sessions vÃ  chuyá»ƒn Ä‘á»•i legacy messages

**TrÃ¡ch nhiá»‡m chÃ­nh:**
- Subscribe tá»›i Redis PubSub channels
- Chuyá»ƒn Ä‘á»•i legacy message format sang streaming format má»›i
- Quáº£n lÃ½ lifecycle streaming (initialize â†’ streaming â†’ complete)
- Äiá»u phá»‘i WebSocket callbacks

```mermaid
stateDiagram-v2
    [*] --> INITIALIZING: startStreamingSession()
    INITIALIZING --> STREAMING: Chunk Ä‘áº§u tiÃªn nháº­n Ä‘Æ°á»£c
    STREAMING --> STREAMING: CÃ¡c sá»± kiá»‡n onChunk()
    STREAMING --> COMPLETED: isComplete = true
    STREAMING --> ERROR: Xáº£y ra lá»—i
    COMPLETED --> [*]
    ERROR --> [*]
```

### 4. AI Service Load Balancer (Java)

**Má»¥c Ä‘Ã­ch:** PhÃ¢n phá»‘i API requests trÃªn cÃ¡c AI service nodes

**Chiáº¿n lÆ°á»£c:** Round-Robin vá»›i retry logic

```mermaid
graph TD
    A[API Request] --> B[AiServiceLoadBalancer]
    B --> C{Chá»n Node Tiáº¿p Theo}
    C --> D[AI Node 1]
    C --> E[AI Node 2]
    C --> F[AI Node 3]
    
    D --> G{ThÃ nh CÃ´ng?}
    E --> G
    F --> G
    
    G -->|CÃ³| H[Tráº£ vá» Response]
    G -->|KhÃ´ng| I{CÃ²n Retry?}
    I -->|CÃ³| C
    I -->|KhÃ´ng| J[Throw Exception]
    
    style B fill:#99ccff
    style C fill:#ffcc99
```

**Triá»ƒn khai:**
```java
// Chá»n round-robin
int index = Math.abs(currentIndex.getAndIncrement() % aiServiceUrls.size());
String url = aiServiceUrls.get(index);

// Retry logic
for (int attempt = 0; attempt < maxRetries; attempt++) {
    try {
        return restTemplate.exchange(url, method, entity, responseType);
    } catch (Exception e) {
        // Thá»­ node tiáº¿p theo
    }
}
```

---

## ğŸ”„ Luá»“ng Xá»­ LÃ½

### Luá»“ng 1: User Gá»­i Tin Nháº¯n Chat

```mermaid
sequenceDiagram
    participant Client as React Frontend
    participant LB as Nginx LB
    participant Backend as Java Backend
    participant AILB as AI Load Balancer
    participant AI as Python AI Service
    participant Redis as Redis PubSub

    Client->>LB: POST /api/chat<br/>{message, session_id}
    LB->>Backend: Route tá»›i Backend Node<br/>(qua sticky session)
    Backend->>AILB: Forward request
    AILB->>AI: POST /chat<br/>(round-robin)
    
    Note over AI: Táº¡o message_id<br/>LÆ°u user message<br/>Báº¯t Ä‘áº§u async streaming
    
    AI-->>AILB: 200 OK<br/>{message_id, status: "streaming"}
    AILB-->>Backend: Response
    Backend-->>LB: Response
    LB-->>Client: Response vá»›i AI message_id
    
    Note over Client: Track message_id<br/>cho cancellation
    
    loop Streaming Chunks
        AI->>Redis: PUBLISH chat:stream:session_id<br/>{chunk, accumulated_content}
        Redis->>Backend: Message broadcast
        Backend->>Client: WebSocket message<br/>{type: "message", data: chunk}
    end
    
    AI->>Redis: PUBLISH complete message
    Redis->>Backend: Complete notification
    Backend->>Client: WebSocket message<br/>{type: "complete", data: final}
```

### Luá»“ng 2: PhÃ¢n Phá»‘i Message Äa Node

```mermaid
sequenceDiagram
    participant Client1 as Client 1
    participant WS1 as Backend Node 1
    participant Client2 as Client 2
    participant WS2 as Backend Node 2
    participant Redis as Redis PubSub
    participant AI as AI Service
    
    Client1->>WS1: Gá»­i message
    WS1->>AI: Forward tá»›i AI (load-balanced)
    
    loop Streaming
        AI->>Redis: PUBLISH tá»›i channel
        Redis-->>WS1: Broadcast
        Redis-->>WS2: Broadcast
        
        Note over WS1: Filter: CÃ³ pháº£i<br/>session cá»§a tÃ´i?
        WS1->>Client1: Gá»­i tá»›i WebSocket
        
        Note over WS2: Filter: CÃ³ pháº£i<br/>session cá»§a tÃ´i?
        WS2->>Client2: Gá»­i tá»›i WebSocket<br/>(náº¿u session tá»“n táº¡i)
    end
```

**Äiá»ƒm chÃ­nh:**
1. Redis PubSub broadcast tá»›i Táº¤T Cáº¢ backend nodes
2. Má»—i node filter messages cho sessions cá»§a nÃ³
3. Chá»‰ relevant messages Ä‘Æ°á»£c gá»­i tá»›i WebSocket clients
4. KhÃ´ng cáº§n giao tiáº¿p giá»¯a cÃ¡c nodes

### Luá»“ng 3: Káº¿t Ná»‘i WebSocket vá»›i Sticky Sessions

```mermaid
sequenceDiagram
    participant Client as Browser
    participant LB as Nginx LB
    participant WS1 as Backend Node 1
    participant WS2 as Backend Node 2
    participant Redis as Redis
    
    Note over Client: IP: 192.168.1.100
    
    Client->>LB: WS Connect<br/>ws://server/ws/chat
    
    Note over LB: ip_hash tÃ­nh:<br/>hash(192.168.1.100) % 3 = 1
    
    LB->>WS1: Route tá»›i Node 1
    WS1->>Redis: ÄÄƒng kÃ½ session
    WS1-->>Client: Káº¿t ná»‘i thÃ nh cÃ´ng
    
    Note over Client,WS1: Connection liÃªn tá»¥c<br/>Ä‘Æ°á»£c duy trÃ¬
    
    Client->>LB: Requests tiáº¿p theo
    
    Note over LB: CÃ¹ng IP â†’ CÃ¹ng hash<br/>LuÃ´n route tá»›i Node 1
    
    LB->>WS1: LuÃ´n lÃ  Node 1
    
    Note over Client,WS1: Má»i traffic tá»« client nÃ y<br/>Ä‘á»u Ä‘i tá»›i cÃ¹ng backend node
```

**Táº¡i sao Sticky Sessions?**
- WebSocket = káº¿t ná»‘i stateful lÃ¢u dÃ i
- Má»—i backend node giá»¯ WebSocket connection trong memory
- Pháº£i luÃ´n route tá»›i node nÆ¡i connection tá»“n táº¡i
- `ip_hash` Ä‘áº£m báº£o Ä‘iá»u nÃ y vá»›i tÃ­nh hash Ä‘Æ¡n giáº£n

### Luá»“ng 4: Há»§y Streaming Message

```mermaid
sequenceDiagram
    participant Client as React Frontend
    participant Backend as Java Backend
    participant AILB as AI Load Balancer
    participant AI as Python AI Service
    participant Redis as Redis
    
    Note over Client: User báº¥m nÃºt Cancel<br/>Trong khi streaming
    
    Client->>Backend: POST /api/cancel<br/>{session_id, message_id}
    Backend->>AILB: Forward cancel request
    AILB->>AI: POST /cancel<br/>(round-robin)
    
    Note over AI: Kiá»ƒm tra active_tasks<br/>ÄÃ¡nh dáº¥u cancelled
    
    AI-->>AILB: 200 OK {status: "cancelled"}
    AILB-->>Backend: Response
    Backend-->>Client: Cancel confirmed
    
    Note over AI: Trong streaming loop:<br/>Kiá»ƒm tra cancelled flag
    
    alt Náº¿u cancelled
        AI->>Redis: PUBLISH cancelled message<br/>{content + "[ÄÃ£ há»§y]", is_complete: true}
        Redis->>Backend: Broadcast
        Backend->>Client: WebSocket: stream dá»«ng
    end
```

---

## ğŸ¯ TÃ­nh NÄƒng ChÃ­nh

### 1. Sticky Sessions qua IP Hash

**CÃ¡ch hoáº¡t Ä‘á»™ng:**
```nginx
upstream websocket_backend {
    ip_hash;  # Hash client IP Ä‘á»ƒ luÃ´n route tá»›i cÃ¹ng server
    server java-websocket-1:8080;
    server java-websocket-2:8080;
    server java-websocket-3:8080;
}
```

**Lá»£i Ã­ch:**
- âœ… KhÃ´ng cáº§n shared session storage cho WebSocket state
- âœ… ÄÆ¡n giáº£n vÃ  hiá»‡u nÄƒng cao (O(1) hash calculation)
- âœ… Hoáº¡t Ä‘á»™ng trong suá»‘t khÃ´ng cáº§n thay Ä‘á»•i client
- âœ… Failover tá»± Ä‘á»™ng (Nginx phÃ¡t hiá»‡n unhealthy nodes)

**CÃ¢n nháº¯c:**
- âš ï¸ Client IP thay Ä‘á»•i â†’ backend node má»›i
- âš ï¸ Load khÃ´ng Ä‘á»u náº¿u IPs khÃ´ng phÃ¢n bá»‘ Ä‘á»“ng Ä‘á»u
- âœ… Cháº¥p nháº­n Ä‘Æ°á»£c cho POC vÃ  háº§u háº¿t production scenarios

### 2. Shared State qua Redis

**Nhá»¯ng gÃ¬ Ä‘Æ°á»£c chia sáº»:**
1. **Session Registry** - Sessions nÃ o Ä‘ang active trÃªn táº¥t cáº£ nodes
2. **Stream Chunks** - Message chunks cÃ³ thá»© tá»± cho recovery
3. **Message History** - Lá»‹ch sá»­ message Ä‘áº§y Ä‘á»§ má»—i session
4. **PubSub Channels** - PhÃ¢n phá»‘i message real-time

**Cáº¥u trÃºc dá»¯ liá»‡u sá»­ dá»¥ng:**
```
sessions:active              â†’ RMap<sessionId, userId>
sessions:user:{userId}       â†’ RSet<sessionId>
stream:chunks:{messageId}    â†’ RList<StreamChunk>
stream:session:{sessionId}   â†’ RHash (session metadata)
chat:stream:{sessionId}      â†’ PubSub Channel
```

**Lá»£i Ã­ch:**
- âœ… Táº¥t cáº£ nodes truy cáº­p cÃ¹ng data
- âœ… Scale ngang khÃ´ng cÃ³ data silos
- âœ… Thao tÃ¡c in-memory nhanh (Redis)
- âœ… Tá»± Ä‘á»™ng expiration qua TTL

### 3. Backend API Gateway Pattern

**Kiáº¿n trÃºc:**
```
Frontend â†’ Nginx â†’ Backend Gateway â†’ AI Services (load-balanced)
```

**Táº¡i sao khÃ´ng truy cáº­p trá»±c tiáº¿p?**
```
Frontend â†’ Nginx â†’ AI Services (trá»±c tiáº¿p)  âŒ
```

**Æ¯u Ä‘iá»ƒm:**
1. **Single Entry Point** - Frontend chá»‰ biáº¿t má»™t endpoint
2. **Security** - Backend cÃ³ thá»ƒ validate, rate-limit, log
3. **Flexibility** - Thay Ä‘á»•i AI services khÃ´ng áº£nh hÆ°á»Ÿng frontend
4. **Load Balancing** - Backend kiá»ƒm soÃ¡t chiáº¿n lÆ°á»£c phÃ¢n phá»‘i
5. **Retry Logic** - Retry tÃ­ch há»£p cho failed AI requests
6. **Health Checks** - Backend giÃ¡m sÃ¡t AI service health

### 4. Real-Time Streaming

**Triá»ƒn khai:**
- AI Service streams tá»«ng tá»« má»™t
- Má»—i chunk Ä‘Æ°á»£c publish tá»›i Redis PubSub
- Backend nodes subscribe vÃ  forward tá»›i WebSocket clients
- Frontend hiá»ƒn thá»‹ incremental updates

**Æ¯u Ä‘iá»ƒm:**
- âœ… UX tá»‘t hÆ¡n (user tháº¥y response ngay láº­p tá»©c)
- âœ… Perceived latency tháº¥p hÆ¡n
- âœ… CÃ³ thá»ƒ cancel giá»¯a chá»«ng
- âœ… Hiá»‡u quáº£ network (incremental transfer)

### 5. Há»— Trá»£ Cancellation

**Luá»“ng:**
1. Frontend track `message_id` tá»« response `/chat`
2. User clicks cancel â†’ POST `/api/cancel` vá»›i `message_id`
3. AI Service Ä‘Ã¡nh dáº¥u streaming task lÃ  cancelled
4. Streaming loop kiá»ƒm tra flag vÃ  dá»«ng
5. Gá»­i final message vá»›i marker "[ÄÃ£ há»§y]"

**Triá»ƒn khai chÃ­nh:**
```python
# Python AI Service
self.active_tasks[session_id] = {
    "message_id": message_id,
    "cancelled": False
}

# Trong streaming loop
if self.active_tasks.get(session_id, {}).get("cancelled", False):
    break  # Dá»«ng streaming
```

### 6. Message Recovery

**Ká»‹ch báº£n:** Client ngáº¯t káº¿t ná»‘i giá»¯a chá»«ng streaming

**Giáº£i phÃ¡p:**
1. Chunks Ä‘Æ°á»£c lÆ°u trong Redis vá»›i TTL (5 phÃºt)
2. Client reconnect vá»›i `lastChunkIndex`
3. Backend láº¥y missing chunks tá»« Redis
4. Resume streaming tá»« vá»‹ trÃ­ cÅ©

**Triá»ƒn khai:**
```java
// Láº¥y chunks tá»« Redis
List<StreamChunk> missingChunks = redisStreamCache.getChunks(
    messageId, 
    lastChunkIndex, 
    currentIndex
);

// Gá»­i tá»›i client
missingChunks.forEach(chunk -> sendChunk(wsSession, chunk));
```

---

## ğŸ’» Triá»ƒn Khai Ká»¹ Thuáº­t

### Technology Stack

#### Frontend
- **React 18** - UI framework
- **Vite** - Build tool (nhanh hÆ¡n webpack)
- **Axios** - HTTP client
- **Native WebSocket API** - Giao tiáº¿p real-time

#### Backend (Java)
- **Spring Boot 3.x** - Application framework
- **Spring WebSocket** - Há»— trá»£ WebSocket
- **Redisson** - Redis client vá»›i distributed primitives
- **Jackson** - JSON serialization
- **H2 Database** - In-memory SQL database (POC)

#### AI Service (Python)
- **FastAPI** - Async web framework
- **Pydantic** - Data validation
- **Redis-py** - Redis client
- **Uvicorn** - ASGI server

#### Háº¡ Táº§ng
- **Nginx** - Load balancer & reverse proxy
- **Redis 7** - In-memory data store & message broker
- **Apache Kafka** - Event streaming (tÃ¹y chá»n)
- **Docker** - Containerization
- **Docker Compose** - Multi-container orchestration

---

## ğŸš¢ Kiáº¿n TrÃºc Triá»ƒn Khai

### Docker Compose Stack

```mermaid
graph TB
    subgraph "Docker Network: app-network (172.20.0.0/16)"
        subgraph "Load Balancer"
            nginx[nginx-lb:80<br/>Public: 8080]
        end
        
        subgraph "Backend Cluster"
            ws1[java-websocket-1:8080]
            ws2[java-websocket-2:8080]
            ws3[java-websocket-3:8080]
        end
        
        subgraph "AI Service Cluster"
            ai1[python-ai-1:8000]
            ai2[python-ai-2:8000]
            ai3[python-ai-3:8000]
        end
        
        subgraph "Infrastructure"
            redis[redis:6379<br/>Public: 6379]
            kafka[kafka:9092<br/>Public: 9092]
        end
        
        subgraph "Frontend"
            frontend[frontend:3000<br/>Public: 3000]
        end
    end
    
    nginx --> ws1
    nginx --> ws2
    nginx --> ws3
    
    ws1 --> ai1
    ws1 --> ai2
    ws1 --> ai3
    
    ws2 --> ai1
    ws2 --> ai2
    ws2 --> ai3
    
    ws3 --> ai1
    ws3 --> ai2
    ws3 --> ai3
    
    ws1 --> redis
    ws2 --> redis
    ws3 --> redis
    
    ai1 --> redis
    ai2 --> redis
    ai3 --> redis
    
    ws1 --> kafka
    ws2 --> kafka
    ws3 --> kafka
    
    frontend -.->|External| nginx
    
    style nginx fill:#ff9999
    style redis fill:#ffcc99
    style kafka fill:#99ccff
```

### Cáº¥u HÃ¬nh Services

| Service | Replicas | CPU | Memory | Ports Exposed |
|---------|----------|-----|--------|---------------|
| Nginx LB | 1 | 0.5 | 128MB | 8080 (HTTP) |
| Java Backend | 3 | 1.0 | 768MB | None (internal) |
| Python AI | 3 | 0.5 | 256MB | None (internal) |
| Redis | 1 | 0.5 | 512MB | 6379 (dev only) |
| Kafka | 1 | 0.5 | 512MB | 9092 (dev only) |
| Frontend | 1 | 0.5 | 128MB | 3000 (HTTP) |

**Tá»•ng TÃ i NguyÃªn:**
- CPU: ~7.5 cores
- Memory: ~4.5GB
- PhÃ¹ há»£p cho phÃ¡t triá»ƒn trÃªn laptop/desktop

---

## âœ… Táº¡i Sao Chá»n Giáº£i PhÃ¡p NÃ y

### 1. CÃ¡c Pattern Kiáº¿n TrÃºc ÄÃ£ ÄÆ°á»£c Chá»©ng Minh

| Pattern | Sá»­ Dá»¥ng Cho | ÄÆ°á»£c Ãp Dá»¥ng Bá»Ÿi |
|---------|----------|-------------------|
| **Sticky Sessions** | WebSocket persistence | Netflix, Slack, Discord |
| **Backend Gateway** | API aggregation | Amazon (API Gateway), Google Cloud |
| **PubSub Messaging** | Real-time events | Twitter, LinkedIn |
| **Distributed Locks** | Data consistency | MongoDB, Elasticsearch |
| **Event Sourcing** | Audit trail | NgÃ¢n hÃ ng, E-commerce |

### 2. Lá»£i Ãch Váº­n HÃ nh

#### Triá»ƒn Khai Dá»… DÃ ng
```bash
# Má»™t lá»‡nh Ä‘á»ƒ start toÃ n bá»™ stack
docker compose -f docker-compose.sticky-session.yml up -d

# Kiá»ƒm tra status
docker compose ps

# Xem logs
docker compose logs -f java-websocket-1

# Scale up
docker compose up -d --scale python-ai=5
```

#### Zero-Code Scaling
- ThÃªm backend nodes â†’ tá»± Ä‘á»™ng load balancing
- ThÃªm AI nodes â†’ chá»‰ update environment variable
- KhÃ´ng cáº§n thay Ä‘á»•i code á»©ng dá»¥ng

#### Monitoring TÃ­ch Há»£p
- Health check endpoints
- Nginx access logs (vá»›i upstream info)
- Kafka topics cho analytics
- Redis monitoring via redis-cli

#### Fault Tolerance
- **Backend node fails** â†’ Nginx routes tá»›i healthy nodes
- **AI node fails** â†’ Load balancer retries trÃªn other nodes
- **Redis fails** â†’ Service giáº£m chá»©c nÄƒng nhÆ°ng khÃ´ng crash
- **Kafka fails** â†’ Chá»©c nÄƒng core (chat) váº«n hoáº¡t Ä‘á»™ng

---

## ğŸš€ Khá»Ÿi Äá»™ng Nhanh

### YÃªu Cáº§u

- Docker & Docker Compose
- Tá»‘i thiá»ƒu 8GB RAM
- 20GB dung lÆ°á»£ng Ä‘Ä©a
- TrÃ¬nh duyá»‡t hiá»‡n Ä‘áº¡i (cho frontend)

### Khá»Ÿi Äá»™ng Há»‡ Thá»‘ng

```bash
# Clone repository
git clone <repo-url>
cd <repo-directory>

# Checkout sticky session branch
git checkout dev_sticky_session

# Build vÃ  start táº¥t cáº£ services
docker compose -f docker-compose.sticky-session.yml up -d --build

# Äá»£i services healthy (30-60 giÃ¢y)
docker compose -f docker-compose.sticky-session.yml ps

# Kiá»ƒm tra logs
docker compose logs -f java-websocket-1 python-ai-1 frontend
```

### Truy Cáº­p á»¨ng Dá»¥ng

| Service | URL | MÃ´ Táº£ |
|---------|-----|-------|
| **Frontend** | http://localhost:3000 | React web application |
| **Backend API** | http://localhost:8080/api | REST API endpoints |
| **WebSocket** | ws://localhost:8080/ws/chat | Káº¿t ná»‘i WebSocket |
| **Health Check** | http://localhost:8080/health | Tráº¡ng thÃ¡i há»‡ thá»‘ng |

### Test Há»‡ Thá»‘ng

#### 1. Má»Ÿ Frontend
Navigate tá»›i http://localhost:3000

#### 2. Gá»­i Message
- Nháº­p message trong chat input
- Click "Send" hoáº·c nháº¥n Enter
- Quan sÃ¡t streaming response

#### 3. Test Cancellation
- Gá»­i má»™t message dÃ i
- Click nÃºt "Cancel" trong khi streaming
- Verify streaming dá»«ng ngay láº­p tá»©c

#### 4. Test Session Persistence
- Refresh trang (F5)
- Verify chat history Ä‘Æ°á»£c khÃ´i phá»¥c
- Verify messages má»›i tiáº¿p tá»¥c trong cÃ¹ng session

#### 5. Test Multi-Node
```bash
# Má»Ÿ nhiá»u browser tabs
# Táº¥t cáº£ tabs káº¿t ná»‘i tá»›i load balancer
# Má»—i tab Ä‘Æ°á»£c sticky session tá»›i má»™t backend node

# Kiá»ƒm tra backend node nÃ o xá»­ lÃ½ tab nÃ o
docker compose logs nginx-lb | grep "upstream:"
```

### Verify Load Balancing

```bash
# Kiá»ƒm tra backend logs Ä‘á»ƒ xem node nÃ o xá»­ lÃ½ session nÃ o
docker compose logs java-websocket-1 | grep "WebSocket connected"
docker compose logs java-websocket-2 | grep "WebSocket connected"
docker compose logs java-websocket-3 | grep "WebSocket connected"

# Kiá»ƒm tra phÃ¢n phá»‘i AI service
docker compose logs java-websocket-1 | grep "AI service request successful"
```

### Dá»«ng Há»‡ Thá»‘ng

```bash
# Dá»«ng táº¥t cáº£ services
docker compose -f docker-compose.sticky-session.yml down

# Dá»«ng vÃ  xÃ³a volumes (clean slate)
docker compose -f docker-compose.sticky-session.yml down -v
```

---

## ğŸ“š TÃ i NguyÃªn Bá»• Sung

### Files Cáº¥u HÃ¬nh

- `docker-compose.sticky-session.yml` - Multi-node orchestration
- `nginx-sticky-session.conf` - Load balancer configuration
- `application.yml` - Backend configuration
- `config.py` - AI service configuration

### Endpoints ChÃ­nh

#### Backend (Java)
- `POST /api/chat` - Gá»­i chat message
- `POST /api/cancel` - Há»§y streaming
- `GET /api/history/{sessionId}` - Láº¥y chat history
- `GET /api/health` - Kiá»ƒm tra sá»©c khá»e AI services
- `GET /actuator/health` - Kiá»ƒm tra sá»©c khá»e Backend

#### AI Service (Python)
- `POST /chat` - Xá»­ lÃ½ chat request
- `POST /cancel` - Há»§y streaming
- `GET /history/{sessionId}` - Láº¥y history tá»« Redis
- `GET /health` - Kiá»ƒm tra sá»©c khá»e Service

### Lá»‡nh Monitoring

```bash
# Kiá»ƒm tra status service
docker compose ps

# Xem logs (táº¥t cáº£ services)
docker compose logs -f

# Xem logs (service cá»¥ thá»ƒ)
docker compose logs -f java-websocket-1

# Kiá»ƒm tra Redis data
docker exec -it sticky-redis redis-cli
> KEYS *
> GET sessions:active

# Kiá»ƒm tra Kafka topics
docker exec -it sticky-kafka kafka-topics.sh --bootstrap-server localhost:9092 --list

# Monitor Nginx connections
docker exec sticky-nginx-lb cat /var/log/nginx/access.log | tail -20

# Kiá»ƒm tra resource usage
docker stats
```

---

## ğŸ¯ Káº¿t Luáº­n

### ThÃ nh Tá»±u

POC nÃ y thÃ nh cÃ´ng chá»©ng minh:

âœ… **Kiáº¿n TrÃºc CÃ³ Thá»ƒ Má»Ÿ Rá»™ng** - Triá»ƒn khai Ä‘a node vá»›i horizontal scaling  
âœ… **Real-Time Streaming** - AI response streaming dá»±a trÃªn WebSocket  
âœ… **Sticky Sessions** - Káº¿t ná»‘i liÃªn tá»¥c qua Nginx ip_hash  
âœ… **Shared State** - Quáº£n lÃ½ session phÃ¢n tÃ¡n qua Redis  
âœ… **Load Balancing** - PhÃ¢n phá»‘i AI service round-robin vá»›i retry  
âœ… **High Availability** - Failover tá»± Ä‘á»™ng vÃ  health checks  
âœ… **Váº­n HÃ nh ÄÆ¡n Giáº£n** - Docker Compose single-command deployment  
âœ… **Developer Experience** - Kiáº¿n trÃºc rÃµ rÃ ng, dá»… má»Ÿ rá»™ng  
