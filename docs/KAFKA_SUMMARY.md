# TÃ³m Táº¯t: Kafka trong Multi-Node Chat Stream Architecture

## ğŸ¯ Táº¡i Sao Cáº§n Kafka?

### Váº¥n Äá» Khi KhÃ´ng CÃ³ Kafka

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Without Kafka (Redis PubSub Only)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  âŒ No audit trail â†’ Cannot debug production issues       â”‚
â”‚  âŒ No analytics â†’ Cannot measure performance              â”‚
â”‚  âŒ Cannot replay â†’ Data lost after TTL                    â”‚
â”‚  âŒ No async processing â†’ Heavy tasks block real-time     â”‚
â”‚  âŒ No guaranteed delivery â†’ Messages can be lost          â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Giáº£i PhÃ¡p Vá»›i Kafka

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  With Kafka (Redis + Kafka)                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  âœ… Complete audit trail â†’ Debug any issue                 â”‚
â”‚  âœ… Real-time analytics â†’ Monitor performance              â”‚
â”‚  âœ… Stream replay â†’ Rebuild data anytime                   â”‚
â”‚  âœ… Async processing â†’ No impact on latency                â”‚
â”‚  âœ… Guaranteed delivery â†’ At-least-once semantics          â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ Kiáº¿n TrÃºc Tá»•ng Quan

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Multi-Node Chat Stream Architecture                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Frontend (React)                                               â”‚
â”‚       â”‚                                                          â”‚
â”‚       â”‚ WebSocket                                                â”‚
â”‚       â–¼                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚  â”‚ Java WS Node â”‚ â—€â”€â”€â”                                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                                          â”‚
â”‚         â”‚            â”‚ Load                                      â”‚
â”‚         â”‚ Publish    â”‚ Balance                                  â”‚
â”‚         â–¼            â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚                                          â”‚
â”‚  â”‚    Redis     â”‚    â”‚                                          â”‚
â”‚  â”‚   PubSub     â”‚â”€â”€â”€â”€â”¤ Real-time (< 100ms)                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                                          â”‚
â”‚         â”‚            â”‚                                          â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚                                                                  â”‚
â”‚         â”‚                                                        â”‚
â”‚         â”‚ Also publish to Kafka (async, no latency impact)     â”‚
â”‚         â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    Kafka Cluster                          â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚  â”‚
â”‚  â”‚  â”‚  chat-events   â”‚  â”‚ stream-events  â”‚                 â”‚  â”‚
â”‚  â”‚  â”‚  Topic         â”‚  â”‚ Topic          â”‚                 â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚             â”‚                   â”‚                              â”‚
â”‚             â”‚ Fan-out to multiple consumers                    â”‚
â”‚             â”‚                   â”‚                              â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚      â”‚      â”‚      â”‚            â”‚        â”‚               â”‚    â”‚
â”‚      â–¼      â–¼      â–¼            â–¼        â–¼               â–¼    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚Audit â”‚â”‚Analytâ”‚â”‚Searchâ”‚  â”‚Email â”‚â”‚ML    â”‚  ...  â”‚Customâ”‚  â”‚
â”‚  â”‚Trail â”‚â”‚ics   â”‚â”‚Index â”‚  â”‚Alert â”‚â”‚Train â”‚       â”‚Workerâ”‚  â”‚
â”‚  â””â”€â”€â”¬â”€â”€â”€â”˜â””â”€â”€â”¬â”€â”€â”€â”˜â””â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚     â”‚       â”‚       â”‚                                         â”‚
â”‚     â–¼       â–¼       â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚  â”‚  DB  â”‚â”‚Metricâ”‚â”‚ ES   â”‚                                     â”‚
â”‚  â”‚ Auditâ”‚â”‚ DB   â”‚â”‚Searchâ”‚                                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š 4 Use Cases Chi Tiáº¿t

### 1ï¸âƒ£ Event Sourcing & Audit Trail

**Má»¥c Ä‘Ã­ch**: LÆ°u táº¥t cáº£ events Ä‘á»ƒ cÃ³ thá»ƒ truy váº¿t vÃ  debug

**Implementation**:
- âœ… `AuditTrailConsumer` - Listen Kafka events
- âœ… `AuditLog` entity - Store in database
- âœ… `AuditLogRepository` - Query audit logs

**Use Cases**:
```sql
-- TÃ¬m táº¥t cáº£ hoáº¡t Ä‘á»™ng cá»§a user
SELECT * FROM audit_logs WHERE user_id = 'user_123';

-- TÃ¬m errors trong 24h qua
SELECT * FROM audit_logs 
WHERE event_type LIKE '%ERROR%' 
AND timestamp >= NOW() - INTERVAL '24 hours';

-- Trace má»™t conversation
SELECT * FROM audit_logs 
WHERE conversation_id = 'conv_abc' 
ORDER BY timestamp;
```

**Benefits**:
- Compliance: Meet regulatory requirements
- Security: Track all user actions
- Debug: Reproduce production issues
- Analytics: User behavior analysis

---

### 2ï¸âƒ£ Async Background Processing

**Má»¥c Ä‘Ã­ch**: Xá»­ lÃ½ heavy tasks khÃ´ng áº£nh hÆ°á»Ÿng real-time latency

**Architecture**:
```
Real-time path:  WebSocket â†’ Redis â†’ Client (< 100ms)
                                      âœ… User sees response
                                      
Background path: Kafka â†’ Workers â†’ Storage (seconds/minutes)
                 â”œâ”€â–¶ Search indexing
                 â”œâ”€â–¶ Analytics aggregation  
                 â”œâ”€â–¶ ML training data
                 â””â”€â–¶ Email notifications
```

**Implementation**:
- âœ… `AnalyticsConsumer` - Real-time metrics
- Custom consumers - Your specific needs

**Example Workers**:

```java
// 1. Search Indexer
@KafkaListener(topics = "chat-events")
public void indexMessage(String event) {
    // Index in Elasticsearch for search
    elasticsearchClient.index(message);
}

// 2. ML Training Data
@KafkaListener(topics = "chat-events")
public void collectTrainingData(String event) {
    // Save to S3 for ML training
    s3Client.putObject(trainingData);
}

// 3. Email Alerts
@KafkaListener(topics = "stream-events")
public void sendAlerts(String event) {
    if (isError(event)) {
        emailService.send("ops-team@example.com", alert);
    }
}
```

**Benefits**:
- Performance: No impact on real-time latency
- Scalability: Scale workers independently
- Reliability: Retry on failure
- Flexibility: Add new workers anytime

---

### 3ï¸âƒ£ Guaranteed Message Delivery

**Má»¥c Ä‘Ã­ch**: KhÃ´ng máº¥t data khi node crash

**How It Works**:

```
Producer Side:
1. Publish event to Kafka
2. Wait for ACK from all replicas (acks=all)
3. Retry if failed (retries=âˆ)
4. Idempotency prevents duplicates

Consumer Side:
1. Read event from Kafka
2. Process business logic
3. Save to database
4. Commit offset (manual ack)
   
   If step 2-3 fail:
   â†’ Don't commit
   â†’ Kafka will redeliver
   â†’ Auto-retry with exponential backoff
```

**Configuration**:
```java
// Producer (already configured)
acks=all                    // Wait for all replicas
retries=Integer.MAX_VALUE   // Retry forever
enable.idempotence=true     // Prevent duplicates

// Consumer (already configured)
enable.auto.commit=false    // Manual commit
auto.offset.reset=earliest  // Start from beginning
```

**Delivery Guarantees**:
- âœ… At-Least-Once (current implementation)
- Message guaranteed to be delivered
- May have duplicates â†’ Need idempotency in consumer

**Benefits**:
- Reliability: No data loss
- Durability: Survives node crashes
- Consistency: Same data across all nodes

---

### 4ï¸âƒ£ Stream Replay & Recovery

**Má»¥c Ä‘Ã­ch**: Time travel Ä‘á»ƒ debug, rebuild, hoáº·c test

**Use Cases**:

#### A. Debug Production Issue

```java
// Replay specific session to see what happened
List<Map<String, Object>> events = 
    replayService.replaySession("session_123");

// Timeline of events:
// 10:30:00 - SESSION_STARTED
// 10:30:02 - CHUNK_RECEIVED (index=0)
// 10:30:03 - STREAM_ERROR (timeout)
// 10:30:05 - RECOVERY_ATTEMPT (success)
// 10:30:10 - STREAM_COMPLETED
```

#### B. Rebuild Index

```java
// Rebuild search index from all historical data
replayService.replayFromTimestamp(
    "chat-events",
    Instant.now().minus(Duration.ofDays(7)),
    event -> {
        if ("CHAT_MESSAGE".equals(event.get("eventType"))) {
            elasticsearchClient.index(event);
        }
    }
);
```

#### C. Backfill New Consumer

```java
// New feature needs historical data
@KafkaListener(
    topics = "chat-events",
    groupId = "new-feature-consumer"  // New group starts from earliest
)
public void processHistoricalData(String event) {
    // Will process all historical events first
    // Then continue with real-time events
}
```

#### D. Test with Production Data

```java
// Replay to staging environment for testing
replayService.replayFromOffset(
    "chat-events",
    partition=0,
    fromOffset=1000,
    toOffset=2000,
    event -> {
        // Test new code with production data
        newFeature.process(event);
    }
);
```

**Benefits**:
- Debug: See exact sequence of events
- Recovery: Rebuild corrupted data
- Testing: Use production data safely
- Backfill: Populate new consumers

---

## ğŸ”„ Message Flow

### Scenario: User gá»­i message "Hello"

```
Step 1: User Input
â”œâ”€â–¶ Frontend sends "Hello" via WebSocket
â”‚
Step 2: Java Server Receives
â”œâ”€â–¶ ChatOrchestrator processes request
â”‚   â”œâ”€â–¶ Publish to Redis PubSub (real-time)
â”‚   â”‚   â””â”€â–¶ WebSocket delivers to client (50ms)
â”‚   â”‚       â””â”€â–¶ âœ… User sees "Hello" instantly
â”‚   â”‚
â”‚   â””â”€â–¶ Publish to Kafka (async, no waiting)
â”‚       â”œâ”€â–¶ Event: SESSION_STARTED
â”‚       â”œâ”€â–¶ Event: CHUNK_RECEIVED (x10)
â”‚       â””â”€â–¶ Event: STREAM_COMPLETED
â”‚
Step 3: Kafka Distributes (background, parallel)
â”œâ”€â–¶ AuditTrailConsumer
â”‚   â””â”€â–¶ Save to audit_logs table
â”‚
â”œâ”€â–¶ AnalyticsConsumer
â”‚   â””â”€â–¶ Update metrics (latency, chunks, etc.)
â”‚
â”œâ”€â–¶ SearchIndexerConsumer (custom)
â”‚   â””â”€â–¶ Index in Elasticsearch
â”‚
â”œâ”€â–¶ MLTrainingConsumer (custom)
â”‚   â””â”€â–¶ Save to S3 for training
â”‚
â””â”€â–¶ NotificationConsumer (custom)
    â””â”€â–¶ Send alerts if error

Total User Latency: 50ms (Kafka processing happens in background)
```

---

## ğŸ“ˆ Metrics & Monitoring

### Kafka Metrics

```
Producer Metrics:
â”œâ”€ kafka.producer.record_send_rate    // Events/second
â”œâ”€ kafka.producer.batch_size_avg      // Batch efficiency
â””â”€ kafka.producer.compression_rate    // Compression ratio

Consumer Metrics:
â”œâ”€ kafka.consumer.records_consumed_rate  // Processing rate
â”œâ”€ kafka.consumer.lag                    // Events behind
â””â”€ kafka.consumer.commit_latency         // Ack speed

Application Metrics:
â”œâ”€ analytics.sessions.started
â”œâ”€ analytics.streams.completed
â”œâ”€ analytics.stream.duration
â”œâ”€ analytics.chunks.received
â”œâ”€ analytics.errors.stream
â””â”€ analytics.recovery.success
```

### Monitoring Dashboard

```bash
# Kafka UI (visual monitoring)
http://localhost:8090

# View:
â”œâ”€ Topics (chat-events, stream-events)
â”œâ”€ Messages in real-time
â”œâ”€ Consumer groups and lag
â”œâ”€ Partitions and offsets
â””â”€ Broker health
```

---

## âš¡ Performance

### Latency Impact

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Without Kafka                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  WebSocket â†’ Process â†’ Send to client                     â”‚
â”‚            â””â”€â–¶ 50ms â—€â”€â”˜                                    â”‚
â”‚                                                            â”‚
â”‚  Total: 50ms âœ…                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  With Kafka (Async Publishing)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  WebSocket â†’ Process â†’ Send to client                     â”‚
â”‚            â””â”€â–¶ 50ms â—€â”€â”˜                                    â”‚
â”‚                 â”‚                                           â”‚
â”‚                 â””â”€â–¶ Kafka.send() (fire and forget, < 1ms) â”‚
â”‚                     â””â”€â–¶ Background workers process later   â”‚
â”‚                                                            â”‚
â”‚  Total: 51ms âœ… (negligible impact)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Throughput

```
Kafka Performance (single broker, 3 partitions):
â”œâ”€ Write: 10,000 events/second
â”œâ”€ Read:  30,000 events/second (3 consumers)
â””â”€ Storage: 1GB = ~500,000 events

Our Usage (POC):
â”œâ”€ Write: ~100 events/second (low volume)
â”œâ”€ Read:  ~300 events/second (3 consumers x 100)
â””â”€ Headroom: 100x capacity available âœ…
```

---

## ğŸ› ï¸ Configuration

### Current Setup (docker-compose.yml)

```yaml
kafka:
  environment:
    # Retention
    KAFKA_LOG_RETENTION_HOURS: 168        # 7 days
    KAFKA_LOG_RETENTION_BYTES: 1073741824 # 1GB
    
    # Replication (single node = 1)
    KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    
    # Auto-create topics
    KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"

java-websocket:
  environment:
    KAFKA_ENABLED: true
    KAFKA_BOOTSTRAP_SERVERS: kafka:9092
```

### Production Recommendations

```yaml
kafka:
  environment:
    # Longer retention for event sourcing
    KAFKA_LOG_RETENTION_HOURS: 8760  # 1 year
    
    # Higher replication for reliability
    KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
    
    # More partitions for scalability
    KAFKA_NUM_PARTITIONS: 6
```

---

## ğŸ“ Best Practices

### Do's âœ…

1. **Use Manual Acknowledgment**
   - Control exactly when message is committed
   - Prevent data loss on processing failure

2. **Implement Idempotency**
   ```java
   if (!isAlreadyProcessed(messageId)) {
       processEvent(event);
       markAsProcessed(messageId);
   }
   ```

3. **Monitor Consumer Lag**
   - Alert if lag > threshold
   - Indicates consumer slower than producer

4. **Use Dead Letter Queue**
   ```java
   catch (Exception e) {
       if (isUnrecoverable(e)) {
           sendToDeadLetterQueue(event);
       }
   }
   ```

5. **Version Your Events**
   ```json
   {
     "eventType": "STREAM_COMPLETED",
     "version": "1.0",
     "data": {...}
   }
   ```

### Don'ts âŒ

1. **Don't Auto-commit**
   - Lose control of delivery guarantees
   - May lose messages on failure

2. **Don't Block in Listener**
   ```java
   // âŒ Bad
   Thread.sleep(10000);  // Causes rebalancing
   
   // âœ… Good
   executorService.submit(() -> heavyWork());
   ```

3. **Don't Ignore Errors**
   ```java
   // âŒ Bad
   catch (Exception e) {
       log.error("Error", e);
       ack.acknowledge();  // Message lost!
   }
   
   // âœ… Good
   catch (Exception e) {
       log.error("Error", e);
       throw e;  // Kafka will retry
   }
   ```

4. **Don't Store Large Payloads**
   ```java
   // âŒ Bad
   event.put("fullContent", largeString);  // > 1MB
   
   // âœ… Good
   event.put("contentRef", s3Key);  // Reference pattern
   ```

---

## ğŸ“š Summary

| Aspect | Redis PubSub | Kafka | Combined (Best) |
|--------|--------------|-------|-----------------|
| Real-time Delivery | âœ… < 100ms | âŒ Slower | âœ… Redis for real-time |
| Guaranteed Delivery | âŒ Can lose | âœ… At-least-once | âœ… Kafka for guarantees |
| Audit Trail | âŒ No history | âœ… Event log | âœ… Kafka for audit |
| Replay Events | âŒ No replay | âœ… Time travel | âœ… Kafka for replay |
| Scalability | âš ï¸ Limited | âœ… Partitions | âœ… Best of both |
| Latency Impact | âœ… None | âš ï¸ Adds latency | âœ… Async = no impact |

### The Perfect Combo ğŸ¯

```
Redis PubSub:
â””â”€â–¶ Real-time delivery (< 100ms latency)
    â””â”€â–¶ User experience âœ…

Kafka:
â”œâ”€â–¶ Event sourcing (audit trail)
â”œâ”€â–¶ Analytics (metrics)
â”œâ”€â–¶ Async processing (no latency impact)
â”œâ”€â–¶ Guaranteed delivery (no data loss)
â””â”€â–¶ Stream replay (debug & recovery)
    â””â”€â–¶ Reliability & observability âœ…
```

**Result**: Fast real-time experience + Complete observability + Zero data loss! ğŸš€
